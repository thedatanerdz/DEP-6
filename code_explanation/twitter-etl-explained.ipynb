{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tweepy\nimport pandas as pd \nimport json\nfrom datetime import datetime\nimport s3fs ","metadata":{"_uuid":"d096f710-b11e-4b1d-b1f8-cca578f10402","_cell_guid":"ae9df86a-3ae0-4117-86e3-17f981c61399","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"import tweepy: This imports the tweepy library, which is a Python wrapper around the Twitter API. It allows you to interact with Twitter's RESTful API to access and work with Twitter data, such as tweets, user information, and timelines.\n\nimport pandas as pd: This imports the pandas library and gives it an alias pd. pandas is a powerful library in Python used for data manipulation and analysis. It provides data structures like DataFrame, which allows you to organize and analyze data efficiently.\n\nimport json: This imports the json module, which provides functions to work with JSON (JavaScript Object Notation) data. JSON is a lightweight data interchange format commonly used for data exchange between a server and a web application, or between different systems.\n\nfrom datetime import datetime: This imports only the datetime class from the datetime module. The datetime class in Python provides functions to work with dates and times, allowing you to perform various operations like parsing, formatting, and arithmetic on date and time objects.\n\nimport s3fs: This imports the s3fs library, which is a Python library that allows you to interact with Amazon S3 (Simple Storage Service). S3 is a cloud-based object storage service provided by Amazon Web Services (AWS). The s3fs library makes it easy to read and write data from and to S3 buckets directly from Python code.","metadata":{}},{"cell_type":"code","source":"def run_twitter_etl():\n\n    access_key = \"<>\" \n    access_secret = \"<>\" \n    consumer_key = \"<>\"\n    consumer_secret = \"<>\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" Based on the variable names used in the function, it appears to be setting up access credentials for Twitter API. The function might be part of an Extract, Transform, Load (ETL) process that involves fetching data from Twitter using the Twitter API.\n\nHowever, the actual implementation of the ETL process is missing in the code snippet you provided. Typically, after setting up the access credentials, you would proceed to use the tweepy library (which is not included in the code) to interact with the Twitter API and retrieve the desired data.\n\nHere's what the code does, based on the variable names:\n\n- access_key: This variable should contain the access key (API key) required to authenticate and access the Twitter API. You obtain this key when you create a Twitter developer account and create an application.\n- access_secret: This variable should contain the access secret (API secret key) associated with the access key. It is another part of the authentication process for accessing the Twitter API.\n- consumer_key: This variable should contain the consumer key (API key) associated with your Twitter developer account. This key is used to identify your application when making requests to the Twitter API.\n- consumer_secret: This variable should contain the consumer secret (API secret key) associated with the consumer key. Similar to the access secret, it is used in the authentication process for API requests.\n\nTo complete the ETL process and fetch data from Twitter, you would need to implement the rest of the run_twitter_etl() function, which would involve using the tweepy library and its functions to interact with the Twitter API, retrieve tweets, users, or other data, and then perform the necessary data transformations and loading steps.\n\nRemember that the full implementation would involve additional code beyond the access credentials to handle API authentication, data retrieval, data processing, and possibly data storage in a database or file system.","metadata":{}},{"cell_type":"code","source":"    # Twitter authentication\n    auth = tweepy.OAuthHandler(access_key, access_secret)   \n    auth.set_access_token(consumer_key, consumer_secret) \n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The code snippet is a continuation of the Python function run_twitter_etl(). It shows the implementation of Twitter authentication using the tweepy library with the access credentials that were defined earlier.\n\nHere's what the code does:\n\n- auth = tweepy.OAuthHandler(access_key, access_secret): This line creates an instance of tweepy.OAuthHandler, which is used to handle the OAuth 1.0a authentication process required by the Twitter API. The OAuthHandler requires two arguments: the access_key and access_secret, which were set up earlier. This step initializes the authentication handler with your application's access credentials.\n- auth.set_access_token(consumer_key, consumer_secret): After creating the OAuthHandler, this line sets the access token and access token secret for API access. The consumer_key and consumer_secret were also set up earlier. These keys, along with the previously provided access credentials, complete the authentication process and allow your application to make authorized requests to the Twitter API on behalf of your Twitter developer account.\n\nWith the authentication successfully set up using the tweepy.OAuthHandler, you can now proceed to use this authenticated auth object to interact with the Twitter API and fetch data from Twitter, such as tweets, users, trends, and more, depending on the specific data you want to extract as part of your ETL process.","metadata":{}},{"cell_type":"code","source":"    # # # Creating an API object \n    api = tweepy.API(auth)\n    tweets = api.user_timeline(screen_name='@elonmusk', \n                            # 200 is the maximum allowed count\n                            count=200,\n                            include_rts = False,\n                            # Necessary to keep full_text \n                            # otherwise only the first 140 words are extracted\n                            tweet_mode = 'extended'\n                            )\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The code snippet continues from the previous implementation and focuses on creating an API object using the authenticated credentials and fetching tweets from the user '@elonmusk'.\n\nHere's what the code does:\n\n- api = tweepy.API(auth): This line creates an API object using the authenticated auth object. The tweepy.API class provides a convenient way to interact with various endpoints of the Twitter API. By passing the auth object, the API object is now authorized to make requests on behalf of the application with the given credentials.\n- tweets = api.user_timeline(screen_name='@elonmusk', count=200, include_rts=False, tweet_mode='extended'): This line fetches the user timeline of the Twitter account with the screen name '@elonmusk'. The parameters used are as follows:\n- screen_name: The Twitter handle of the user for whom the timeline is being retrieved. In this case, it's set to '@elonmusk', the handle of Elon Musk.\n- count: The number of tweets to retrieve. The code is set to fetch up to 200 tweets at once, which is the maximum allowed count per request.\n- include_rts: A Boolean flag indicating whether to include retweets in the retrieved timeline. In this case, it's set to False, meaning retweets will not be included.\n- tweet_mode: The tweet_mode is set to 'extended' to ensure that the full_text of each tweet is extracted, as opposed to just the truncated text. Twitter's standard tweet length is 280 characters, and by using tweet_mode='extended', you can retrieve tweets beyond the 140 characters limit.\n\nAfter running this code, the tweets variable will contain up to 200 tweets from the user '@elonmusk'. Each tweet will have attributes such as created_at (timestamp), full_text (the full content of the tweet), and other metadata associated with each tweet.\n\nKeep in mind that Twitter's API may have rate limits, meaning you can only make a certain number of requests per window of time. If you need to fetch more tweets or perform other operations, you might need to handle rate limiting and pagination accordingly.","metadata":{}},{"cell_type":"code","source":"    list = []\n    for tweet in tweets:\n        text = tweet._json[\"full_text\"]\n\n        refined_tweet = {\"user\": tweet.user.screen_name,\n                        'text' : text,\n                        'favorite_count' : tweet.favorite_count,\n                        'retweet_count' : tweet.retweet_count,\n                        'created_at' : tweet.created_at}\n        \n        list.append(refined_tweet)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The provided code snippet processes the list of tweets obtained from the previous API call and refines each tweet by extracting specific information from them. It then creates a list of dictionaries containing the refined information for each tweet.\n\nHere's what the code does:\n\n- list = []: This line initializes an empty list named list. Note that it is generally not recommended to use built-in names like list for variable names to avoid potential conflicts.\n- for tweet in tweets:: This line starts a loop that iterates through each tweet in the tweets list. Each tweet represents a single tweet from the user timeline.\n- text = tweet._json[\"full_text\"]: This line extracts the full text of the tweet from the tweet's JSON data. The tweet._json attribute provides the raw JSON representation of the tweet, and [\"full_text\"] is used to access the full text of the tweet.\n- refined_tweet = {...}: This block of code refines the tweet information by creating a dictionary named refined_tweet. It contains the following key-value pairs:\n  -\"user\": The screen name of the user who tweeted the tweet (e.g., '@elonmusk').\n  - \"text\": The full text of the tweet extracted in step 3.\n  - \"favorite_count\": The number of times the tweet has been favorited (liked) by other users.\n  - \"retweet_count\": The number of times the tweet has been retweeted by other users.\n  - \"created_at\": The timestamp representing when the tweet was created.\n- list.append(refined_tweet): This line adds the refined_tweet dictionary, representing a single tweet, to the list. The loop will continue iterating through all the tweets in tweets, refining each tweet and appending its dictionary representation to the list.\n\nAfter running this code, the list will contain a list of dictionaries, where each dictionary represents a refined tweet with the user's screen name, full text, favorite count, retweet count, and creation timestamp.\n\nKeep in mind that this code example only processes the first 200 tweets fetched from the user '@elonmusk'. If you have more tweets to process or need to handle rate limiting and pagination, additional logic may be required.","metadata":{}},{"cell_type":"code","source":"    df = pd.DataFrame(list)\n    df.to_csv('refined_tweets.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The provided code snippet converts the list of refined tweets, created in the previous steps, into a Pandas DataFrame and then exports it to a CSV file named \"refined_tweets.csv\".\n\nHere's what the code does:\n\n- df = pd.DataFrame(list): This line creates a Pandas DataFrame named df from the list of refined tweets. Each dictionary in the list corresponds to a row in the DataFrame, and the keys of the dictionaries become column names. The DataFrame will have columns: \"user\", \"text\", \"favorite_count\", \"retweet_count\", and \"created_at\".\n- df.to_csv('refined_tweets.csv'): This line exports the DataFrame df to a CSV file named \"refined_tweets.csv\". The CSV file will be created in the current working directory (the directory from which the Python script was executed). Each row in the DataFrame will be written as a separate line in the CSV file, with values separated by commas.\n\nAfter running this code, you will have a CSV file named \"refined_tweets.csv\" containing the refined tweet data in a structured tabular format. You can then use this CSV file for further analysis, visualization, or sharing the data with others.\n\nIt's worth mentioning that exporting the DataFrame to CSV is just one way to save the data. Depending on your needs, you could also use other formats like JSON, Excel, or databases for data storage and sharing.","metadata":{}}]}